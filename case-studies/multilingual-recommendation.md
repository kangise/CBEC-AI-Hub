# å¤šè¯­è¨€äº§å“æ¨èç³»ç»Ÿ - æŠ€æœ¯æ–¹æ¡ˆç¤ºä¾‹

> **ğŸ“ é‡è¦è¯´æ˜**: è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–¹æ¡ˆç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æ„å»ºå¤šè¯­è¨€æ¨èç³»ç»Ÿçš„å®Œæ•´æŠ€æœ¯æ¶æ„ã€‚æ–‡ä¸­çš„æ€§èƒ½æ•°æ®å’Œä¸šåŠ¡æŒ‡æ ‡ä»…ä¸ºç¤ºä¾‹å‚è€ƒï¼Œå®é™…é¡¹ç›®æ•ˆæœä¼šå› æ•°æ®åˆ†å¸ƒã€ç”¨æˆ·è¡Œä¸ºç­‰å› ç´ è€Œå¼‚ã€‚

## é¡¹ç›®æ¦‚è¿°

æœ¬æŠ€æœ¯æ–¹æ¡ˆå±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªæ”¯æŒå¤šè¯­è¨€å’Œè·¨æ–‡åŒ–çš„äº§å“æ¨èç³»ç»Ÿï¼Œä¸ºå…¨çƒåŒ–ç”µå•†å¹³å°æä¾›ä¸ªæ€§åŒ–æ¨èæœåŠ¡çš„æŠ€æœ¯å‚è€ƒã€‚

## ä¸šåŠ¡èƒŒæ™¯

### æŒ‘æˆ˜
- **è¯­è¨€éšœç¢**: ç”¨æˆ·ä½¿ç”¨ä¸åŒè¯­è¨€æœç´¢å’Œæµè§ˆäº§å“
- **æ–‡åŒ–å·®å¼‚**: ä¸åŒåœ°åŒºç”¨æˆ·çš„è´­ä¹°åå¥½å’Œè¡Œä¸ºæ¨¡å¼å·®å¼‚å·¨å¤§
- **å†·å¯åŠ¨é—®é¢˜**: æ–°ç”¨æˆ·å’Œæ–°äº§å“ç¼ºä¹å†å²æ•°æ®
- **æ•°æ®ç¨€ç–æ€§**: è·¨è¯­è¨€å’Œè·¨åœ°åŒºçš„äº¤äº’æ•°æ®ç¨€ç–

### é¢„æœŸä¸šåŠ¡ç›®æ ‡
- æé«˜ç”¨æˆ·å‚ä¸åº¦å’Œè½¬åŒ–ç‡
- å¢å¼ºç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦
- æ‰©å¤§äº§å“è¦†ç›–èŒƒå›´
- æ”¯æŒä¸šåŠ¡å…¨çƒåŒ–æ‰©å¼ 

> **æ³¨**: ä»¥ä¸‹æŠ€æœ¯æ–¹æ¡ˆåŸºäºæ¨èç³»ç»Ÿé¢†åŸŸçš„æœ€ä½³å®è·µè®¾è®¡

## æŠ€æœ¯æ–¹æ¡ˆ

### ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A[ç”¨æˆ·è¡Œä¸ºæ•°æ®] --> B[å¤šè¯­è¨€æ–‡æœ¬å¤„ç†]
    C[äº§å“ä¿¡æ¯] --> B
    B --> D[è·¨è¯­è¨€åµŒå…¥]
    D --> E[ç”¨æˆ·ç”»åƒæ„å»º]
    D --> F[äº§å“è¡¨ç¤ºå­¦ä¹ ]
    E --> G[æ¨èæ¨¡å‹]
    F --> G
    H[æ–‡åŒ–åå¥½æ¨¡å‹] --> G
    G --> I[å€™é€‰ç”Ÿæˆ]
    I --> J[æ’åºä¼˜åŒ–]
    J --> K[å¤šæ ·æ€§è°ƒæ•´]
    K --> L[æ¨èç»“æœ]
    
    M[A/Bæµ‹è¯•æ¡†æ¶] --> J
    N[å®æ—¶åé¦ˆ] --> E
```

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

```python
# ä¸»è¦ä¾èµ–
lightfm==1.16
spacy==3.4.1
sentence-transformers==2.2.2
scikit-learn==1.1.2
pandas==1.4.3
numpy==1.23.2
mlflow==1.28.0
fastapi==0.85.0
redis==4.3.4
```

## å®ç°ç»†èŠ‚

### 1. å¤šè¯­è¨€æ–‡æœ¬å¤„ç†

```python
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np

class MultilingualTextProcessor:
    def __init__(self):
        # åŠ è½½å¤šè¯­è¨€æ¨¡å‹
        self.nlp_models = {
            'en': spacy.load('en_core_web_sm'),
            'zh': spacy.load('zh_core_web_sm'),
            'es': spacy.load('es_core_news_sm'),
            'fr': spacy.load('fr_core_news_sm'),
            'de': spacy.load('de_core_news_sm'),
            'ja': spacy.load('ja_core_news_sm')
        }
        
        # å¤šè¯­è¨€å¥å­åµŒå…¥æ¨¡å‹
        self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    def detect_language(self, text):
        """è¯­è¨€æ£€æµ‹"""
        from langdetect import detect
        try:
            return detect(text)
        except:
            return 'en'  # é»˜è®¤è‹±è¯­
    
    def preprocess_text(self, text, language=None):
        """æ–‡æœ¬é¢„å¤„ç†"""
        if language is None:
            language = self.detect_language(text)
        
        if language not in self.nlp_models:
            language = 'en'
        
        nlp = self.nlp_models[language]
        doc = nlp(text)
        
        # æå–å…³é”®è¯å’Œå®ä½“
        keywords = [token.lemma_.lower() for token in doc 
                   if not token.is_stop and not token.is_punct and token.is_alpha]
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        
        return {
            'keywords': keywords,
            'entities': entities,
            'language': language,
            'processed_text': ' '.join(keywords)
        }
    
    def get_text_embedding(self, text):
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        return self.sentence_model.encode([text])[0]
    
    def compute_text_similarity(self, text1, text2):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        emb1 = self.get_text_embedding(text1)
        emb2 = self.get_text_embedding(text2)
        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
```

### 2. è·¨æ–‡åŒ–ç”¨æˆ·å»ºæ¨¡

```python
from lightfm import LightFM
from lightfm.data import Dataset
import pandas as pd

class CrossCulturalUserModel:
    def __init__(self):
        self.text_processor = MultilingualTextProcessor()
        self.cultural_features = {
            'US': {'individualism': 0.91, 'uncertainty_avoidance': 0.46, 'power_distance': 0.40},
            'CN': {'individualism': 0.20, 'uncertainty_avoidance': 0.30, 'power_distance': 0.80},
            'DE': {'individualism': 0.67, 'uncertainty_avoidance': 0.65, 'power_distance': 0.35},
            'JP': {'individualism': 0.46, 'uncertainty_avoidance': 0.92, 'power_distance': 0.54},
            'BR': {'individualism': 0.38, 'uncertainty_avoidance': 0.76, 'power_distance': 0.69}
        }
    
    def build_user_features(self, user_data):
        """æ„å»ºç”¨æˆ·ç‰¹å¾"""
        features = []
        
        for _, user in user_data.iterrows():
            user_features = []
            
            # åŸºç¡€ç‰¹å¾
            user_features.extend([
                f"age_group:{self._get_age_group(user['age'])}",
                f"gender:{user['gender']}",
                f"country:{user['country']}",
                f"language:{user['preferred_language']}"
            ])
            
            # æ–‡åŒ–ç»´åº¦ç‰¹å¾
            if user['country'] in self.cultural_features:
                cultural = self.cultural_features[user['country']]
                for dim, value in cultural.items():
                    user_features.append(f"cultural_{dim}:{self._discretize(value)}")
            
            # è¡Œä¸ºç‰¹å¾
            user_features.extend([
                f"avg_order_value:{self._discretize_price(user['avg_order_value'])}",
                f"purchase_frequency:{self._get_frequency_group(user['purchase_frequency'])}",
                f"preferred_categories:{','.join(user['preferred_categories'])}"
            ])
            
            features.append(user_features)
        
        return features
    
    def build_item_features(self, product_data):
        """æ„å»ºäº§å“ç‰¹å¾"""
        features = []
        
        for _, product in product_data.iterrows():
            item_features = []
            
            # åŸºç¡€ç‰¹å¾
            item_features.extend([
                f"category:{product['category']}",
                f"brand:{product['brand']}",
                f"price_range:{self._discretize_price(product['price'])}",
                f"rating_range:{self._discretize_rating(product['avg_rating'])}"
            ])
            
            # æ–‡æœ¬ç‰¹å¾
            text_info = self.text_processor.preprocess_text(
                product['title'] + ' ' + product['description']
            )
            
            # æ·»åŠ å…³é”®è¯ç‰¹å¾
            for keyword in text_info['keywords'][:10]:  # å–å‰10ä¸ªå…³é”®è¯
                item_features.append(f"keyword:{keyword}")
            
            # æ·»åŠ è¯­è¨€ç‰¹å¾
            item_features.append(f"content_language:{text_info['language']}")
            
            # åœ°åŒºé€‚åº”æ€§ç‰¹å¾
            if 'target_regions' in product:
                for region in product['target_regions']:
                    item_features.append(f"target_region:{region}")
            
            features.append(item_features)
        
        return features
    
    def _get_age_group(self, age):
        if age < 25: return "young"
        elif age < 35: return "adult"
        elif age < 50: return "middle_aged"
        else: return "senior"
    
    def _discretize(self, value, bins=5):
        return int(value * bins)
    
    def _discretize_price(self, price):
        if price < 20: return "low"
        elif price < 100: return "medium"
        elif price < 500: return "high"
        else: return "premium"
    
    def _discretize_rating(self, rating):
        if rating < 3.0: return "low"
        elif rating < 4.0: return "medium"
        else: return "high"
    
    def _get_frequency_group(self, frequency):
        if frequency < 2: return "occasional"
        elif frequency < 5: return "regular"
        else: return "frequent"
```

### 3. æ¨èæ¨¡å‹è®­ç»ƒ

```python
class MultilingualRecommendationModel:
    def __init__(self, no_components=100, loss='warp', learning_rate=0.05):
        self.model = LightFM(
            no_components=no_components,
            loss=loss,
            learning_rate=learning_rate,
            random_state=42
        )
        self.dataset = Dataset()
        self.user_model = CrossCulturalUserModel()
        self.is_fitted = False
    
    def prepare_data(self, interactions_df, users_df, items_df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # æ„å»ºç”¨æˆ·å’Œç‰©å“ç‰¹å¾
        user_features = self.user_model.build_user_features(users_df)
        item_features = self.user_model.build_item_features(items_df)
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset.fit(
            users=interactions_df['user_id'].unique(),
            items=interactions_df['item_id'].unique(),
            user_features=set(feature for features in user_features for feature in features),
            item_features=set(feature for features in item_features for feature in features)
        )
        
        # æ„å»ºäº¤äº’çŸ©é˜µ
        (interactions, weights) = self.dataset.build_interactions(
            [(row['user_id'], row['item_id'], row['rating']) 
             for _, row in interactions_df.iterrows()]
        )
        
        # æ„å»ºç‰¹å¾çŸ©é˜µ
        user_features_matrix = self.dataset.build_user_features(
            [(users_df.iloc[i]['user_id'], user_features[i]) 
             for i in range(len(users_df))]
        )
        
        item_features_matrix = self.dataset.build_item_features(
            [(items_df.iloc[i]['item_id'], item_features[i]) 
             for i in range(len(items_df))]
        )
        
        return interactions, user_features_matrix, item_features_matrix
    
    def train(self, interactions_df, users_df, items_df, epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
        interactions, user_features, item_features = self.prepare_data(
            interactions_df, users_df, items_df
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(
            interactions,
            user_features=user_features,
            item_features=item_features,
            epochs=epochs,
            verbose=True
        )
        
        self.is_fitted = True
        return self
    
    def predict(self, user_id, item_ids, user_features=None, item_features=None):
        """é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„åå¥½åˆ†æ•°"""
        if not self.is_fitted:
            raise ValueError("Model must be trained before making predictions")
        
        user_internal_id = self.dataset.mapping()[0][user_id]
        item_internal_ids = [self.dataset.mapping()[2][item_id] for item_id in item_ids]
        
        scores = self.model.predict(
            user_internal_id,
            item_internal_ids,
            user_features=user_features,
            item_features=item_features
        )
        
        return scores
    
    def recommend(self, user_id, n_items=10, filter_seen=True):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        if not self.is_fitted:
            raise ValueError("Model must be trained before making recommendations")
        
        user_internal_id = self.dataset.mapping()[0][user_id]
        n_items_total = len(self.dataset.mapping()[2])
        
        scores = self.model.predict(
            user_internal_id,
            np.arange(n_items_total)
        )
        
        # è·å–top-Næ¨è
        top_items = np.argsort(-scores)[:n_items]
        
        # è½¬æ¢å›åŸå§‹ID
        item_mapping = {v: k for k, v in self.dataset.mapping()[2].items()}
        recommended_items = [item_mapping[item] for item in top_items]
        recommended_scores = scores[top_items]
        
        return list(zip(recommended_items, recommended_scores))
```

### 4. å®æ—¶æ¨èæœåŠ¡

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import json
import time

app = FastAPI(title="Multilingual Recommendation API")
redis_client = redis.Redis(host='localhost', port=6379, db=0)

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
recommendation_model = MultilingualRecommendationModel()
recommendation_model.load_model('models/multilingual_recommender.pkl')

class RecommendationRequest(BaseModel):
    user_id: str
    language: str = 'en'
    country: str = 'US'
    n_items: int = 10
    category_filter: list = None

class RecommendationResponse(BaseModel):
    user_id: str
    recommendations: list
    language: str
    processing_time: float
    model_version: str

@app.post("/recommend", response_model=RecommendationResponse)
async def get_recommendations(request: RecommendationRequest):
    """è·å–ä¸ªæ€§åŒ–æ¨è"""
    start_time = time.time()
    
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"rec:{request.user_id}:{request.language}:{request.country}"
        cached_result = redis_client.get(cache_key)
        
        if cached_result:
            recommendations = json.loads(cached_result)
        else:
            # ç”Ÿæˆæ¨è
            raw_recommendations = recommendation_model.recommend(
                request.user_id, 
                n_items=request.n_items * 2  # ç”Ÿæˆæ›´å¤šå€™é€‰ï¼Œåç»­è¿‡æ»¤
            )
            
            # åº”ç”¨è¿‡æ»¤å’Œå¤šæ ·æ€§è°ƒæ•´
            recommendations = await apply_filters_and_diversity(
                raw_recommendations, 
                request
            )
            
            # ç¼“å­˜ç»“æœï¼ˆ1å°æ—¶ï¼‰
            redis_client.setex(cache_key, 3600, json.dumps(recommendations))
        
        processing_time = time.time() - start_time
        
        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=recommendations[:request.n_items],
            language=request.language,
            processing_time=processing_time,
            model_version="v1.2.0"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def apply_filters_and_diversity(recommendations, request):
    """åº”ç”¨è¿‡æ»¤å™¨å’Œå¤šæ ·æ€§è°ƒæ•´"""
    filtered_recs = []
    categories_seen = set()
    
    for item_id, score in recommendations:
        # è·å–ç‰©å“ä¿¡æ¯
        item_info = await get_item_info(item_id)
        
        # ç±»åˆ«è¿‡æ»¤
        if request.category_filter and item_info['category'] not in request.category_filter:
            continue
        
        # å¤šæ ·æ€§æ§åˆ¶ï¼šé™åˆ¶åŒä¸€ç±»åˆ«çš„ç‰©å“æ•°é‡
        if item_info['category'] in categories_seen and len([r for r in filtered_recs if r['category'] == item_info['category']]) >= 2:
            continue
        
        categories_seen.add(item_info['category'])
        
        # æœ¬åœ°åŒ–è°ƒæ•´
        localized_info = await localize_item_info(item_info, request.language, request.country)
        
        filtered_recs.append({
            'item_id': item_id,
            'score': float(score),
            'title': localized_info['title'],
            'description': localized_info['description'],
            'price': localized_info['price'],
            'currency': localized_info['currency'],
            'category': item_info['category'],
            'image_url': item_info['image_url'],
            'rating': item_info['rating'],
            'availability': localized_info['availability']
        })
    
    return filtered_recs

async def get_item_info(item_id):
    """è·å–ç‰©å“ä¿¡æ¯"""
    # ä»æ•°æ®åº“æˆ–ç¼“å­˜è·å–ç‰©å“ä¿¡æ¯
    cache_key = f"item:{item_id}"
    cached_info = redis_client.get(cache_key)
    
    if cached_info:
        return json.loads(cached_info)
    
    # ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    item_info = {
        'item_id': item_id,
        'title': 'Sample Product',
        'description': 'Sample Description',
        'category': 'Electronics',
        'price': 99.99,
        'currency': 'USD',
        'rating': 4.5,
        'image_url': 'https://example.com/image.jpg'
    }
    
    # ç¼“å­˜ç‰©å“ä¿¡æ¯
    redis_client.setex(cache_key, 7200, json.dumps(item_info))
    
    return item_info

async def localize_item_info(item_info, language, country):
    """æœ¬åœ°åŒ–ç‰©å“ä¿¡æ¯"""
    localized_info = item_info.copy()
    
    # ä»·æ ¼æœ¬åœ°åŒ–
    if country != 'US':
        localized_info['price'] = await convert_currency(item_info['price'], 'USD', get_currency(country))
        localized_info['currency'] = get_currency(country)
    
    # æ–‡æœ¬æœ¬åœ°åŒ–ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…åº”è¯¥è°ƒç”¨ç¿»è¯‘æœåŠ¡ï¼‰
    if language != 'en':
        localized_info['title'] = await translate_text(item_info['title'], 'en', language)
        localized_info['description'] = await translate_text(item_info['description'], 'en', language)
    
    # å¯ç”¨æ€§æ£€æŸ¥
    localized_info['availability'] = await check_availability(item_info['item_id'], country)
    
    return localized_info

def get_currency(country):
    """è·å–å›½å®¶å¯¹åº”çš„è´§å¸"""
    currency_map = {
        'US': 'USD', 'CN': 'CNY', 'DE': 'EUR', 
        'JP': 'JPY', 'GB': 'GBP', 'BR': 'BRL'
    }
    return currency_map.get(country, 'USD')

async def convert_currency(amount, from_currency, to_currency):
    """è´§å¸è½¬æ¢ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # å®é™…åº”è¯¥è°ƒç”¨æ±‡ç‡API
    rates = {'USD': 1.0, 'CNY': 6.8, 'EUR': 0.85, 'JPY': 110, 'GBP': 0.75, 'BRL': 5.2}
    return amount * rates.get(to_currency, 1.0) / rates.get(from_currency, 1.0)

async def translate_text(text, from_lang, to_lang):
    """æ–‡æœ¬ç¿»è¯‘ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # å®é™…åº”è¯¥è°ƒç”¨ç¿»è¯‘API
    return f"[{to_lang}] {text}"

async def check_availability(item_id, country):
    """æ£€æŸ¥å•†å“åœ¨æŒ‡å®šå›½å®¶çš„å¯ç”¨æ€§"""
    # å®é™…åº”è¯¥æ£€æŸ¥åº“å­˜å’Œé…é€æ”¿ç­–
    return True

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": time.time()}
```

## é¢„æœŸæ€§èƒ½è¯„ä¼°

> **âš ï¸ å…è´£å£°æ˜**: ä»¥ä¸‹æ€§èƒ½æŒ‡æ ‡ä¸ºåŸºäºæ¨èç³»ç»Ÿç ”ç©¶å’Œè¡Œä¸šç»éªŒçš„é¢„ä¼°å€¼ï¼Œå®é™…æ•ˆæœä¼šå› æ•°æ®è´¨é‡ã€ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ã€ä¸šåŠ¡åœºæ™¯ç­‰å› ç´ è€Œæœ‰æ˜¾è‘—å·®å¼‚ã€‚

### ç›®æ ‡ç¦»çº¿è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡èŒƒå›´ | è¯´æ˜ |
|------|----------|------|
| Precision@10 | 0.10-0.20 | å–å†³äºæ•°æ®ç¨€ç–åº¦å’Œæ¨¡å‹å¤æ‚åº¦ |
| Recall@10 | 0.05-0.15 | å—é™äºå€™é€‰é›†å¤§å°å’Œç”¨æˆ·å…´è¶£å¹¿åº¦ |
| NDCG@10 | 0.15-0.30 | è€ƒè™‘æ’åºè´¨é‡çš„ç»¼åˆæŒ‡æ ‡ |
| Coverage | 0.60-0.80 | æ¨èç³»ç»Ÿè¦†ç›–çš„å•†å“æ¯”ä¾‹ |
| Diversity | 0.70-0.85 | æ¨èç»“æœçš„å¤šæ ·æ€§ç¨‹åº¦ |

### é¢„æœŸåœ¨çº¿æ•ˆæœ

| æŒ‡æ ‡ | åŸºå‡†å€¼ | ç›®æ ‡æå‡ | è¯´æ˜ |
|------|--------|----------|------|
| ç‚¹å‡»ç‡ (CTR) | åŸºå‡† | +15-30% | å–å†³äºåŸºå‡†ç³»ç»Ÿè´¨é‡ |
| è½¬åŒ–ç‡ | åŸºå‡† | +10-25% | å—äº§å“è´¨é‡å’Œä»·æ ¼å½±å“ |
| å¹³å‡è®¢å•ä»·å€¼ | åŸºå‡† | +5-15% | é€šè¿‡äº¤å‰é”€å”®å®ç° |
| ç”¨æˆ·æ»¡æ„åº¦ | åŸºå‡† | +0.2-0.5åˆ† | éœ€è¦ç”¨æˆ·è°ƒç ”éªŒè¯ |
| é¡µé¢åœç•™æ—¶é—´ | åŸºå‡† | +20-40% | åæ˜ ç”¨æˆ·å‚ä¸åº¦ |

### å¤šè¯­è¨€æ€§èƒ½é¢„æœŸ

| è¯­è¨€ | æ•°æ®å……è¶³åº¦ | é¢„æœŸPrecision@10 | æŒ‘æˆ˜ |
|------|------------|------------------|------|
| è‹±è¯­ | é«˜ | 0.15-0.20 | ç«äº‰æ¿€çƒˆï¼Œç”¨æˆ·æœŸæœ›é«˜ |
| ä¸­æ–‡ | é«˜ | 0.12-0.18 | æ–‡åŒ–å·®å¼‚ï¼Œåœ°åŸŸåå¥½ |
| è¥¿ç­ç‰™è¯­ | ä¸­ | 0.10-0.15 | åœ°åŒºå·®å¼‚å¤§ |
| æ³•è¯­ | ä¸­ | 0.08-0.14 | æ•°æ®ç›¸å¯¹ç¨€ç– |
| å¾·è¯­ | ä¸­ | 0.08-0.14 | ç”¨æˆ·è¡Œä¸ºä¿å®ˆ |
| æ—¥è¯­ | ä½ | 0.06-0.12 | æ–‡åŒ–ç‰¹æ®Šæ€§å¼º |

## ä¼˜åŒ–ç­–ç•¥

### 1. å†·å¯åŠ¨é—®é¢˜è§£å†³

```python
class ColdStartHandler:
    def __init__(self, recommendation_model):
        self.model = recommendation_model
        self.popularity_model = PopularityBasedRecommender()
        self.content_model = ContentBasedRecommender()
    
    def handle_new_user(self, user_profile):
        """å¤„ç†æ–°ç”¨æˆ·å†·å¯åŠ¨"""
        # åŸºäºäººå£ç»Ÿè®¡å­¦ç‰¹å¾çš„æ¨è
        demographic_recs = self.get_demographic_recommendations(user_profile)
        
        # åŸºäºåœ°ç†ä½ç½®çš„æµè¡Œå•†å“æ¨è
        popular_recs = self.popularity_model.recommend_by_region(
            user_profile['country'], 
            user_profile['language']
        )
        
        # æ··åˆæ¨è
        return self.blend_recommendations([demographic_recs, popular_recs], [0.6, 0.4])
    
    def handle_new_item(self, item_info):
        """å¤„ç†æ–°å•†å“å†·å¯åŠ¨"""
        # åŸºäºå†…å®¹çš„ç›¸ä¼¼å•†å“æ¨è
        similar_items = self.content_model.find_similar_items(item_info)
        
        # åŸºäºç±»åˆ«çš„æ¨èç­–ç•¥
        category_strategy = self.get_category_strategy(item_info['category'])
        
        return {
            'similar_items': similar_items,
            'promotion_strategy': category_strategy
        }
```

### 2. å®æ—¶ä¸ªæ€§åŒ–

```python
class RealTimePersonalization:
    def __init__(self):
        self.session_tracker = SessionTracker()
        self.real_time_updater = RealTimeModelUpdater()
    
    def update_recommendations(self, user_id, interaction_data):
        """åŸºäºå®æ—¶äº¤äº’æ›´æ–°æ¨è"""
        # æ›´æ–°ç”¨æˆ·ä¼šè¯çŠ¶æ€
        session_state = self.session_tracker.update_session(user_id, interaction_data)
        
        # å®æ—¶è°ƒæ•´æ¨èæƒé‡
        adjusted_weights = self.calculate_dynamic_weights(session_state)
        
        # é‡æ–°æ’åºæ¨èç»“æœ
        return self.rerank_recommendations(user_id, adjusted_weights)
    
    def calculate_dynamic_weights(self, session_state):
        """è®¡ç®—åŠ¨æ€æƒé‡"""
        weights = {
            'popularity': 0.3,
            'collaborative': 0.4,
            'content': 0.2,
            'trending': 0.1
        }
        
        # æ ¹æ®ä¼šè¯è¡Œä¸ºè°ƒæ•´æƒé‡
        if session_state['browse_time'] > 300:  # é•¿æ—¶é—´æµè§ˆ
            weights['content'] += 0.1
            weights['popularity'] -= 0.1
        
        if session_state['category_focus']:  # ä¸“æ³¨ç‰¹å®šç±»åˆ«
            weights['content'] += 0.15
            weights['collaborative'] -= 0.15
        
        return weights
```

### 3. å¤šç›®æ ‡ä¼˜åŒ–

```python
class MultiObjectiveOptimizer:
    def __init__(self):
        self.objectives = {
            'relevance': 0.4,
            'diversity': 0.2,
            'novelty': 0.15,
            'business_value': 0.25
        }
    
    def optimize_recommendations(self, candidate_items, user_profile):
        """å¤šç›®æ ‡ä¼˜åŒ–æ¨èç»“æœ"""
        scores = {}
        
        for item in candidate_items:
            scores[item['item_id']] = {
                'relevance': self.calculate_relevance_score(item, user_profile),
                'diversity': self.calculate_diversity_score(item, candidate_items),
                'novelty': self.calculate_novelty_score(item, user_profile),
                'business_value': self.calculate_business_value(item)
            }
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        final_scores = {}
        for item_id, item_scores in scores.items():
            final_score = sum(
                item_scores[obj] * weight 
                for obj, weight in self.objectives.items()
            )
            final_scores[item_id] = final_score
        
        # æ’åºå¹¶è¿”å›
        sorted_items = sorted(
            candidate_items, 
            key=lambda x: final_scores[x['item_id']], 
            reverse=True
        )
        
        return sorted_items
```

## éƒ¨ç½²å’Œç›‘æ§

### ç”Ÿäº§ç¯å¢ƒæ¶æ„

```yaml
# kubernetes-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multilingual-recommender
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multilingual-recommender
  template:
    metadata:
      labels:
        app: multilingual-recommender
    spec:
      containers:
      - name: recommender-api
        image: cbec-ai/multilingual-recommender:v1.2.0
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: MODEL_PATH
          value: "/models/multilingual_recommender.pkl"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: model-storage
          mountPath: /models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: recommender-service
spec:
  selector:
    app: multilingual-recommender
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### ç›‘æ§æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge

# ä¸šåŠ¡æŒ‡æ ‡
recommendation_requests = Counter('recommendation_requests_total', 'Total recommendation requests', ['language', 'country'])
recommendation_ctr = Gauge('recommendation_ctr', 'Click-through rate', ['language'])
recommendation_conversion = Gauge('recommendation_conversion_rate', 'Conversion rate', ['language'])

# æŠ€æœ¯æŒ‡æ ‡
recommendation_latency = Histogram('recommendation_latency_seconds', 'Recommendation latency')
model_accuracy = Gauge('model_accuracy', 'Model accuracy score', ['metric'])
cache_hit_rate = Gauge('cache_hit_rate', 'Cache hit rate')

@app.middleware("http")
async def monitor_requests(request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # è®°å½•å»¶è¿Ÿ
    latency = time.time() - start_time
    recommendation_latency.observe(latency)
    
    return response
```

## æ€»ç»“

æœ¬æŠ€æœ¯æ–¹æ¡ˆå±•ç¤ºäº†æ„å»ºå¤šè¯­è¨€äº§å“æ¨èç³»ç»Ÿçš„å®Œæ•´æŠ€æœ¯è·¯å¾„ï¼Œå…³é”®æŠ€æœ¯è¦ç‚¹åŒ…æ‹¬ï¼š

1. **å¤šè¯­è¨€æ”¯æŒ**: ä½¿ç”¨å…ˆè¿›çš„å¤šè¯­è¨€NLPæ¨¡å‹
2. **æ–‡åŒ–é€‚åº”**: é›†æˆæ–‡åŒ–ç»´åº¦ç‰¹å¾
3. **å†·å¯åŠ¨å¤„ç†**: å¤šç­–ç•¥è§£å†³æ–°ç”¨æˆ·å’Œæ–°å•†å“é—®é¢˜
4. **å®æ—¶ä¼˜åŒ–**: åŸºäºç”¨æˆ·è¡Œä¸ºå®æ—¶è°ƒæ•´æ¨è
5. **å¤šç›®æ ‡å¹³è¡¡**: åœ¨ç›¸å…³æ€§ã€å¤šæ ·æ€§å’Œå•†ä¸šä»·å€¼é—´æ‰¾åˆ°å¹³è¡¡

### å®æ–½å»ºè®®

- **æ•°æ®æ”¶é›†**: å»ºè®®æ¯ç§è¯­è¨€è‡³å°‘æ”¶é›†10ä¸‡+ç”¨æˆ·äº¤äº’æ•°æ®
- **æ¨¡å‹è®­ç»ƒ**: å¯é‡‡ç”¨è¿ç§»å­¦ä¹ ï¼Œä»æ•°æ®ä¸°å¯Œçš„è¯­è¨€è¿ç§»åˆ°æ•°æ®ç¨€ç–çš„è¯­è¨€
- **A/Bæµ‹è¯•**: å»ºè®®è¿›è¡Œè‡³å°‘4å‘¨çš„A/Bæµ‹è¯•éªŒè¯æ•ˆæœ
- **ç›‘æ§ä½“ç³»**: é‡ç‚¹ç›‘æ§ä¸åŒè¯­è¨€å’Œåœ°åŒºçš„æ€§èƒ½å·®å¼‚

### æŠ€æœ¯æ ˆæ›¿ä»£æ–¹æ¡ˆ

- **æ¨èç®—æ³•**: å¯é€‰æ‹©Neural Collaborative Filteringã€DeepFMç­‰æ·±åº¦å­¦ä¹ æ–¹æ³•
- **å¤šè¯­è¨€æ¨¡å‹**: å¯ä½¿ç”¨XLM-Rã€mBERTç­‰é¢„è®­ç»ƒæ¨¡å‹
- **å®æ—¶æœåŠ¡**: å¯ä½¿ç”¨Apache Kafka + Apache Flinkè¿›è¡Œå®æ—¶è®¡ç®—
- **ç‰¹å¾å­˜å‚¨**: å¯ä½¿ç”¨Feastã€Tectonç­‰ç‰¹å¾å­˜å‚¨ç³»ç»Ÿ

### æ½œåœ¨æŒ‘æˆ˜

- **æ•°æ®ä¸å¹³è¡¡**: ä¸åŒè¯­è¨€çš„æ•°æ®é‡å·®å¼‚å·¨å¤§
- **æ–‡åŒ–å·®å¼‚**: éœ€è¦æ·±å…¥ç†è§£å„åœ°åŒºç”¨æˆ·è¡Œä¸ºæ¨¡å¼
- **å†·å¯åŠ¨**: æ–°å¸‚åœºå’Œæ–°ç”¨æˆ·çš„æ¨èè´¨é‡éš¾ä»¥ä¿è¯
- **å®æ—¶æ€§**: å¤§è§„æ¨¡å¤šè¯­è¨€æ¨èçš„å»¶è¿Ÿæ§åˆ¶

> **ğŸ’¡ è´¡çŒ®é‚€è¯·**: å¦‚æœæ‚¨æœ‰å¤šè¯­è¨€æ¨èç³»ç»Ÿçš„å®é™…é¡¹ç›®ç»éªŒï¼Œæ¬¢è¿åˆ†äº«çœŸå®æ¡ˆä¾‹ã€é‡åˆ°çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼

## ç›¸å…³èµ„æº

- [æºä»£ç ä»“åº“](https://github.com/cbec-ai-hub/multilingual-recommender)
- [æ¨¡å‹è®­ç»ƒç¬”è®°](https://github.com/cbec-ai-hub/multilingual-recommender/blob/main/notebooks/model_training.ipynb)
- [APIæ–‡æ¡£](https://api.example.com/recommender/docs)
- [æ€§èƒ½åŸºå‡†æµ‹è¯•](https://github.com/cbec-ai-hub/multilingual-recommender/blob/main/benchmarks/)